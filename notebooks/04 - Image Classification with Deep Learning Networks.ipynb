{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Workshop: How to develop a personalised machine learning-based application\n",
    "#\n",
    "# Notebook 4: Image Classification with Deep Learning Networks\n",
    "#\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook instructions:\n",
    "# - Every cell can be executed seperately from the rest.\n",
    "# - You can execute cells in a non-sequential order (but be carefull of \n",
    "#   the dependencies between them).\n",
    "# - Execute a cell by pressing the play button or Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can play with a mini deep learning model with Tensorflow's characteristics in the following URL\n",
    "# http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = '../data/train'\n",
    "validation_data_dir = '../data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify the model's structure\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(3, img_width, img_height)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1984/2000 [============================>.] - ETA: 1s - loss: 0.7536 - acc: 0.5267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angelos/anaconda2/lib/python2.7/site-packages/keras/engine/training.py:1403: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002/2000 [==============================] - 189s - loss: 0.7533 - acc: 0.5255 - val_loss: 0.6904 - val_acc: 0.5363\n",
      "Epoch 2/50\n",
      "2002/2000 [==============================] - 192s - loss: 0.6914 - acc: 0.5415 - val_loss: 0.6627 - val_acc: 0.5975\n",
      "Epoch 3/50\n",
      "2002/2000 [==============================] - 200s - loss: 0.6672 - acc: 0.5969 - val_loss: 0.6807 - val_acc: 0.5637\n",
      "Epoch 4/50\n",
      "2002/2000 [==============================] - 156s - loss: 0.6368 - acc: 0.6464 - val_loss: 0.6703 - val_acc: 0.5938\n",
      "Epoch 5/50\n",
      "2002/2000 [==============================] - 141s - loss: 0.6167 - acc: 0.6653 - val_loss: 0.5851 - val_acc: 0.6863\n",
      "Epoch 6/50\n",
      "2002/2000 [==============================] - 150s - loss: 0.5912 - acc: 0.7023 - val_loss: 0.5606 - val_acc: 0.7050\n",
      "Epoch 7/50\n",
      "2002/2000 [==============================] - 149s - loss: 0.5705 - acc: 0.7058 - val_loss: 0.5896 - val_acc: 0.7025\n",
      "Epoch 8/50\n",
      "2002/2000 [==============================] - 150s - loss: 0.5656 - acc: 0.7163 - val_loss: 0.7430 - val_acc: 0.6500\n",
      "Epoch 9/50\n",
      "2002/2000 [==============================] - 147s - loss: 0.5469 - acc: 0.7248 - val_loss: 0.5212 - val_acc: 0.7412\n",
      "Epoch 10/50\n",
      "2002/2000 [==============================] - 137s - loss: 0.5164 - acc: 0.7522 - val_loss: 0.5398 - val_acc: 0.7375\n",
      "Epoch 11/50\n",
      "2002/2000 [==============================] - 134s - loss: 0.5072 - acc: 0.7493 - val_loss: 0.5186 - val_acc: 0.7538\n",
      "Epoch 12/50\n",
      "2002/2000 [==============================] - 144s - loss: 0.4806 - acc: 0.7727 - val_loss: 0.6516 - val_acc: 0.7000\n",
      "Epoch 13/50\n",
      "2002/2000 [==============================] - 141s - loss: 0.5108 - acc: 0.7627 - val_loss: 0.5133 - val_acc: 0.7488\n",
      "Epoch 14/50\n",
      "2002/2000 [==============================] - 144s - loss: 0.4893 - acc: 0.7662 - val_loss: 0.5383 - val_acc: 0.7375\n",
      "Epoch 15/50\n",
      "2002/2000 [==============================] - 144s - loss: 0.4734 - acc: 0.7802 - val_loss: 0.5545 - val_acc: 0.7625\n",
      "Epoch 16/50\n",
      "2002/2000 [==============================] - 138s - loss: 0.4753 - acc: 0.7737 - val_loss: 0.5402 - val_acc: 0.7325\n",
      "Epoch 17/50\n",
      "2002/2000 [==============================] - 150s - loss: 0.4618 - acc: 0.7882 - val_loss: 0.5556 - val_acc: 0.7512\n",
      "Epoch 18/50\n",
      "2002/2000 [==============================] - 138s - loss: 0.4494 - acc: 0.7947 - val_loss: 0.5805 - val_acc: 0.6863\n",
      "Epoch 19/50\n",
      "2002/2000 [==============================] - 146s - loss: 0.4548 - acc: 0.7887 - val_loss: 0.6154 - val_acc: 0.7312\n",
      "Epoch 20/50\n",
      "2002/2000 [==============================] - 136s - loss: 0.4273 - acc: 0.7957 - val_loss: 0.4817 - val_acc: 0.7612\n",
      "Epoch 21/50\n",
      "2002/2000 [==============================] - 148s - loss: 0.4263 - acc: 0.8057 - val_loss: 0.4744 - val_acc: 0.7775\n",
      "Epoch 22/50\n",
      "2002/2000 [==============================] - 140s - loss: 0.4259 - acc: 0.8037 - val_loss: 0.5232 - val_acc: 0.7662\n",
      "Epoch 23/50\n",
      "2002/2000 [==============================] - 145s - loss: 0.4304 - acc: 0.8112 - val_loss: 0.4726 - val_acc: 0.7850\n",
      "Epoch 24/50\n",
      "2002/2000 [==============================] - 148s - loss: 0.4080 - acc: 0.8097 - val_loss: 0.7381 - val_acc: 0.6887\n",
      "Epoch 25/50\n",
      "2002/2000 [==============================] - 158s - loss: 0.4037 - acc: 0.8242 - val_loss: 0.5261 - val_acc: 0.7538\n",
      "Epoch 26/50\n",
      "2002/2000 [==============================] - 133s - loss: 0.4129 - acc: 0.8127 - val_loss: 0.5051 - val_acc: 0.7700\n",
      "Epoch 27/50\n",
      "2002/2000 [==============================] - 132s - loss: 0.4006 - acc: 0.8297 - val_loss: 0.7695 - val_acc: 0.6875\n",
      "Epoch 28/50\n",
      "2002/2000 [==============================] - 131s - loss: 0.4132 - acc: 0.8137 - val_loss: 0.4573 - val_acc: 0.7762\n",
      "Epoch 29/50\n",
      "2002/2000 [==============================] - 128s - loss: 0.4092 - acc: 0.8197 - val_loss: 0.4979 - val_acc: 0.7675\n",
      "Epoch 30/50\n",
      "2002/2000 [==============================] - 139s - loss: 0.3879 - acc: 0.8312 - val_loss: 0.4702 - val_acc: 0.7750\n",
      "Epoch 31/50\n",
      "2002/2000 [==============================] - 138s - loss: 0.3970 - acc: 0.8222 - val_loss: 0.5711 - val_acc: 0.7575\n",
      "Epoch 32/50\n",
      "2002/2000 [==============================] - 135s - loss: 0.3893 - acc: 0.8282 - val_loss: 0.4798 - val_acc: 0.7625\n",
      "Epoch 33/50\n",
      "2002/2000 [==============================] - 138s - loss: 0.3641 - acc: 0.8467 - val_loss: 0.6047 - val_acc: 0.7650\n",
      "Epoch 34/50\n",
      "2002/2000 [==============================] - 149s - loss: 0.3842 - acc: 0.8397 - val_loss: 0.5717 - val_acc: 0.7425\n",
      "Epoch 35/50\n",
      "2002/2000 [==============================] - 158s - loss: 0.3727 - acc: 0.8397 - val_loss: 0.4965 - val_acc: 0.7887\n",
      "Epoch 36/50\n",
      "2002/2000 [==============================] - 138s - loss: 0.3777 - acc: 0.8367 - val_loss: 0.5379 - val_acc: 0.7700\n",
      "Epoch 37/50\n",
      "2002/2000 [==============================] - 133s - loss: 0.3672 - acc: 0.8506 - val_loss: 0.4605 - val_acc: 0.7863\n",
      "Epoch 38/50\n",
      "2002/2000 [==============================] - 136s - loss: 0.3702 - acc: 0.8487 - val_loss: 0.6213 - val_acc: 0.7600\n",
      "Epoch 39/50\n",
      "2002/2000 [==============================] - 137s - loss: 0.3742 - acc: 0.8332 - val_loss: 0.4940 - val_acc: 0.7700\n",
      "Epoch 40/50\n",
      "2002/2000 [==============================] - 136s - loss: 0.3694 - acc: 0.8442 - val_loss: 0.4899 - val_acc: 0.7688\n",
      "Epoch 41/50\n",
      "2002/2000 [==============================] - 132s - loss: 0.3526 - acc: 0.8631 - val_loss: 0.5025 - val_acc: 0.7875\n",
      "Epoch 42/50\n",
      "2002/2000 [==============================] - 140s - loss: 0.3328 - acc: 0.8566 - val_loss: 0.5657 - val_acc: 0.7800\n",
      "Epoch 43/50\n",
      "2002/2000 [==============================] - 132s - loss: 0.3746 - acc: 0.8382 - val_loss: 0.6868 - val_acc: 0.7725\n",
      "Epoch 44/50\n",
      "2002/2000 [==============================] - 143s - loss: 0.3393 - acc: 0.8581 - val_loss: 0.6519 - val_acc: 0.7987\n",
      "Epoch 45/50\n",
      "2002/2000 [==============================] - 127s - loss: 0.3489 - acc: 0.8497 - val_loss: 0.4826 - val_acc: 0.8013\n",
      "Epoch 46/50\n",
      "2002/2000 [==============================] - 132s - loss: 0.3393 - acc: 0.8487 - val_loss: 0.5392 - val_acc: 0.7800\n",
      "Epoch 47/50\n",
      "2002/2000 [==============================] - 136s - loss: 0.3532 - acc: 0.8596 - val_loss: 0.4723 - val_acc: 0.7825\n",
      "Epoch 48/50\n",
      "2002/2000 [==============================] - 138s - loss: 0.3366 - acc: 0.8536 - val_loss: 0.5319 - val_acc: 0.8075\n",
      "Epoch 49/50\n",
      "2002/2000 [==============================] - 152s - loss: 0.3428 - acc: 0.8656 - val_loss: 0.7047 - val_acc: 0.7600\n",
      "Epoch 50/50\n",
      "2002/2000 [==============================] - 135s - loss: 0.3367 - acc: 0.8686 - val_loss: 0.5014 - val_acc: 0.7837\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)\n",
    "\n",
    "model.save_weights('cats_dogs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
